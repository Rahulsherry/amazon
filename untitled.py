# -*- coding: utf-8 -*-
"""Untitled

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r-6jV2VNqyBBljVaT-c7mOfP-TYqLgG4
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

Data=pd.read_csv('/content/AMZN.csv')
Data.head()

Data.info()

Data.describe()

Data.head()

Data.tail()

Data.isnull().sum()

Data.shape

Data.columns

Data.dtypes

Data.size

Data

"""PRE PROCESSING DATA"""

Data[['Date', 'Adj Close']].plot(figsize=(18, 8))

plt.title('Amazon stock Prices')
plt.xlabel('Date')
plt.ylabel('Price')
plt.gca().invert_xaxis()

plt.show()

Data[['Date', 'Close']].plot(figsize=(18, 8))

plt.title('Amazon stock Prices')
plt.xlabel('Date')
plt.ylabel('Price')
plt.gca().invert_xaxis()

plt.show()

Data['Date'] = pd.to_datetime(Data['Date'])

# Lets plot the date against the close .
plt.plot(Data['Date'] , Data['Close'])

Data = pd.DataFrame(Data)
Data.hist(figsize=(10, 6))
plt.tight_layout()
plt.show()

from plotly.express import imshow
imshow(img=Data.corr(numeric_only=True))

Data.columns.to_list()

import plotly.graph_objects as go # import the graph_objects module and alias it as 'go'
import pandas as pd
Data1 = {
    'Date': pd.date_range('2023-01-01', periods=100),
    'Close': pd.Series(range(100))
}

Data2 = pd.DataFrame(Data1) # Use Data1 which has the date range and close prices
fig = go.Figure()
fig.add_trace(go.Scatter(x=Data2['Date'], y=Data2['Close'], mode='lines', name='Close'))
fig.update_layout(title='Stock Close Prices Over Time',
                 xaxis_title='Date',
                 yaxis_title='Close Price')

fig.show()

#Data2.head()

numeric_Data = Data.select_dtypes(include=[np.number])
correlation_matrix = Data.corr()
plt.figure(figsize=(10, 12))
sns.heatmap(correlation_matrix, annot=True, cmap='viridis', linewidths=0.5)
plt.title('Correlation Heatmap')
plt.show()

numeric_Data

Data.describe().T.plot(kind='bar')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from pandas.plotting import autocorrelation_plot

Dates = pd.date_range(start='2023-01-01', periods=100, freq='D') # Changed 'Date_range' to 'date_range'
Data = np.random.randn(100).cumsum()  # Random cumulative sum to simulate time series data
time_series = pd.DataFrame(Data, index=Dates, columns=['Value']) # Changed 'data' to 'Data' and 'dates' to 'Dates'

print(time_series.head())

plt.figure(figsize=(10,6))
plt.plot(time_series, label='Value')
plt.title('Sample Time Series Data')
plt.xlabel('Date')
plt.ylabel('Value')
plt.legend()
plt.show()

from statsmodels.tsa.seasonal import seasonal_decompose

decomposition = seasonal_decompose(time_series, model='additive', period=30)
decomposition.plot()
plt.show()

autocorrelation_plot(time_series)
plt.show()

from statsmodels.tsa.arima.model import ARIMA

model = ARIMA(time_series, order=(5,1,0))  # order(p,d,q)
arima_result = model.fit()

forecast = arima_result.forecast(steps=40)
print(forecast)

#plt.figure(figsize=(10,6))
#plt.plot(time_series, label='Original')
#plt.plot(pd.date_range(start=Dates[-1], periods=10, freq='D'), forecast, label='Forecast', color='red')
#plt.title('ARIMA Forecast')
#plt.legend()
#plt.show()

plt.figure(figsize=(10,6))
plt.plot(time_series, label='Original')
# Changed periods to 40 to match the forecast length
plt.plot(pd.date_range(start=Dates[-1], periods=40, freq='D'), forecast, label='Forecast', color='red')
plt.title('ARIMA Forecast')
plt.legend()
plt.show()

# Convert the NumPy array to a Pandas DataFrame with a 'Date' column
Data = pd.DataFrame(Data, columns=['Date'])

# Convert the 'Date' column to datetime objects
Data['Date'] = pd.to_datetime(Data['Date'])

Data = Data.sort_values(by='Date')

Data.isnull().sum()

import seaborn as sns
sns.heatmap(Data.isnull(), cbar=False)

Data.interpolate(method='linear', inplace=True)

Data.fillna(method='ffill', inplace=True)  # Forward Fill
Data.fillna(method='bfill', inplace=True)  # Backward Fill

Data.dropna(inplace=True)

# It seems like there might be a typo and you meant to use 'Date' instead of 'Close
# If you have a 'Close' column, make sure it exists and is spelled correctly.
Q1 = Data['Date'].quantile(0.25)
Q3 = Data['Date'].quantile(0.75)  # Use 'Date' here
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Also, the column used here was incorrect
outliers = Data[(Data['Date'] < lower_bound) | (Data['Date'] > upper_bound)]

Data = Data[(Data['Date'] >= lower_bound) & (Data['Date'] <= upper_bound)]

# Convert 'Date' column to datetime
Data['Date'] = pd.to_datetime(Data['Date'])

# Set 'Date' column as index
Data = Data.set_index('Date')

# Now you can resample
Data = Data.resample('D').mean()
Data.fillna(method='ffill', inplace=True)

# Remove 'on='Date' since 'Date' is now the index
Data = Data.resample('W').mean()

Data = Data.apply(pd.to_numeric, errors='coerce')
Data.dropna(inplace=True)  # In case any non-numeric values remain

# Check if Data is empty
if Data.empty:
    print("DataFrame is empty. Check previous operations.")
else:
    # Check if 'value' column exists
    if 'value' not in Data.columns:
        print("Column 'value' not found in DataFrame.")
    else:
        # Perform the ADF test
        adf_result = adfuller(Data['value'].dropna())

        print('ADF Statistic:', adf_result[0])
        print('p-value:', adf_result[1])

# Check if 'Date' column exists before trying to access it
if 'Date' in Data.columns:
    Data['Date'] = pd.to_datetime(Data['Date'])
    Data.set_index('Date', inplace=True)
else:
    print("Column 'Date' not found in DataFrame. Check previous operations or data loading.")

# Remove 'on='Date' since 'Date' is now the index
Data = Data.resample('W').mean()
print(Data) # Print the contents of Data after resampling

Data = Data.apply(pd.to_numeric, errors='coerce')
print(Data) # Print the contents of Data after applying pd.to_numeric

Data.dropna(inplace=True)  # In case any non-numeric values remain
print(Data) # Print the contents of Data after dropping NA values

# ... (rest of your code)

# Plot the decomposition results for additive
plt.figure(figsize=(10, 8))
decomposition.plot() # Changed result_additive to decomposition
plt.suptitle('Additive Decomposition', fontsize=16)
plt.show()

# Perform multiplicative decomposition
from statsmodels.tsa.seasonal import seasonal_decompose

# Check if Data is empty and has the 'value' column
if not Data.empty and 'value' in Data.columns:
    result_multiplicative = seasonal_decompose(Data['value'], model='multiplicative', period=52)

    # Plot the decomposition results for multiplicative
    plt.figure(figsize=(10, 8))
    result_multiplicative.plot()
    plt.suptitle('Multiplicative Decomposition', fontsize=16)
    plt.show()
else:
    print("Data is empty or does not have 'value' column. Check previous operations or data loading.")

# Sample time series data
Data = pd.Series([135, 146, 142, 148, 159, 162, 172, 180, 182, 188, 198])

# ADF Test
def adf_test(series):
    result = adfuller(series)
    print('ADF Statistic:', result[0])
    print('p-value:', result[1])
    if result[1] < 0.05:
        print("Series is stationary")
    else:
        print("Series is non-stationary")

# KPSS Test
from statsmodels.tsa.stattools import kpss

def kpss_test(series):
    statistic, p_value, n_lags, critical_values = kpss(series, regression='c')
    print(f'KPSS Statistic: {statistic}')
    print(f'p-value: {p_value}')
    if p_value < 0.05:
        print("Series is non-stationary")
    else:
        print("Series is stationary")

# Run tests
print("ADF Test:")
adf_test(Data)

print("\nKPSS Test:")
kpss_test(Data)

# If the series is non-stationary, apply differencing or log transformation
Data_diff = Data.diff().dropna()
Data_log = np.log(Data)

# Plot original and transformed series
plt.figure(figsize=(10,6))
plt.subplot(3,1,1)
plt.plot(Data, label='Original Series')
plt.title('Original Time Series')

plt.subplot(3,1,2)
plt.plot(Data_diff, label='Differenced Series')
plt.title('Differenced Time Series')

plt.subplot(3,1,3)
plt.plot(Data_log, label='Log Transformed Series')
plt.title('Log Transformed Time Series')

plt.tight_layout()
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split

# Sample time series data (replace with your actual data)
Data = pd.Series([135, 146, 142, 148, 159, 162, 172, 180, 182, 188, 198])

# Split data into training and testing sets (80% training, 20% testing)
train_size = int(len(Data) * 0.8)
train, test = Data[:train_size], Data[train_size:]

# Plot the data
import matplotlib.pyplot as plt

plt.plot(train, label="Train")
plt.plot(test, label="Test")
plt.title("Train-Test Split")
plt.legend()
plt.show()

from statsmodels.tsa.arima.model import ARIMA

# Define the order of the ARIMA model
p = 1  # Example value for the autoregressive order
d = 0  # Example value for the differencing order
q = 1  # Example value for the moving average order

model_arima = ARIMA(train, order=(p, d, q))
arima_fit = model_arima.fit()
predictions = arima_fit.forecast(steps=len(test))

from statsmodels.tsa.statespace.sarimax import SARIMAX

# Define the seasonal order of the SARIMA model
P = 1  # Example value for the seasonal autoregressive order
D = 0  # Example value for the seasonal differencing order
Q = 1  # Example value for the seasonal moving average order
m = 12 # Example value for the seasonal periodicity

model_sarima = SARIMAX(train, order=(p,d,q), seasonal_order=(P,D,Q,m))
sarima_fit = model_sarima.fit()
predictions = sarima_fit.forecast(steps=len(test))

from statsmodels.tsa.holtwinters import ExponentialSmoothing

# Check if the training data has at least two seasonal cycles
if len(train) >= 2 * 12:
    model_exp = ExponentialSmoothing(train, seasonal='add', trend='add', seasonal_periods=12)
    exp_fit = model_exp.fit()
    predictions = exp_fit.forecast(len(test))
else:
    print("Insufficient data for reliable seasonal modeling. Consider using a different method or gathering more data.")

# Prepare the data in Prophet format
!pip install fbprophet
from prophet import Prophet # import Prophet class from fbprophet module
train_Data = pd.DataFrame({'ds': pd.date_range('2023-01-01', periods=len(train)), 'y': train.values}) # Create a date range for the 'ds' column
model_prophet = Prophet()
model_prophet.fit(train_Data) # use train_Data variable, not train_df

# Forecast future
future = model_prophet.make_future_dataframe(periods=len(test))
forecast = model_prophet.predict(future)

from sklearn.metrics import mean_absolute_error, mean_squared_error

# Example: Evaluate ARIMA model predictions
mae = mean_absolute_error(test, predictions)
rmse = mean_squared_error(test, predictions, squared=False)

print(f"MAE: {mae}")
print(f"RMSE: {rmse}")

from statsmodels.tsa.arima.model import ARIMA

# Define ARIMA model parameters (initial guess)
p, d, q = 1, 1, 1

# Train the ARIMA model on training data
model_arima = ARIMA(train, order=(p, d, q))
arima_fit = model_arima.fit()

# Forecast on test data
predictions = arima_fit.forecast(steps=len(test))

# Evaluate model performance (using metrics like MAE, RMSE)
from sklearn.metrics import mean_squared_error, mean_absolute_error

mae = mean_absolute_error(test, predictions)
rmse = mean_squared_error(test, predictions, squared=False)

print(f"MAE: {mae}")
print(f"RMSE: {rmse}")

# Import the itertools module
import itertools

# Define parameter ranges for p, d, q
p = d = q = range(0, 3)
pdq_combinations = list(itertools.product(p, d, q))

# Grid Search for ARIMA parameters
best_aic = np.inf
best_order = None

for pdq in pdq_combinations:
    try:
        model = ARIMA(train, order=pdq)
        model_fit = model.fit()
        if model_fit.aic < best_aic:
            best_aic = model_fit.aic
            best_order = pdq
    except:
        continue

 #Refit model with best parameters
best_model_arima = ARIMA(train, order=best_order)
best_arima_fit = best_model_arima.fit()#Added this block to ensure the grid search is actually happening
predictions = best_arima_fit.forecast(steps=len(test))

from sklearn.model_selection import TimeSeriesSplit

# Split the data into time-based folds
tscv = TimeSeriesSplit(n_splits=5)

# Example of applying time series split
for train_index, test_index in tscv.split(Data):
    train_fold, test_fold = Data[train_index], Data[test_index]

# Example of applying time series split
for train_index, test_index in tscv.split(Data):
    train_fold, test_fold = Data[train_index], Data[test_index]
    # Forecast and evaluate on the test fold
    predictions = arima_fit.forecast(steps=len(test_fold)) #Removed the extra indent
    mae = mean_absolute_error(test_fold, predictions)
    print(f'MAE for this fold: {mae}')

!pip install scikit-optimize # Install the skopt library

from skopt import gp_minimize #Import the necessary functions from the library
from skopt.space import Integer
from skopt.utils import use_named_args

# Define the hyperparameter space for ARIMA (p, d, q)
search_space = [
    Integer(0, 5, name='p'),
    Integer(0, 2, name='d'),
    Integer(0, 5, name='q')]

# Define the objective function to minimize (using AIC as a metric)
@use_named_args(search_space)
def arima_objective(params):
    p, d, q = params
    try:
        model_arima = ARIMA(train, order=(p, d, q))
        arima_fit = model_arima.fit()
        return arima_fit.aic
    except:
        return np.inf

# Perform Bayesian optimization
#result = gp_minimize(arima_objective, search_space, n_calls=20, random_state=0)

print(f"Best ARIMA order: {result.x} with AIC: {result.fun}")

# Refit model with the best parameters
best_p, best_d, best_q = result.x
best_model_arima = ARIMA(train, order=(best_p, best_d, best_q))
best_arima_fit = best_model_arima.fit()
predictions = best_arima_fit.forecast(steps=len(test))

from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.stats.diagnostic import acorr_ljungbox, het_arch
from scipy import stats

# Assuming 'arima_fit' is the fitted ARIMA model from the previous step
residuals = arima_fit.resid

# Plot residuals
plt.figure(figsize=(12, 6))
plt.plot(residuals)
plt.title('Residuals')
plt.show()

# Plot histogram and QQ plot to check for normality
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.hist(residuals, bins=30)
plt.title('Histogram of Residuals')

plt.subplot(1, 2, 2)
stats.probplot(residuals, dist="norm", plot=plt)
plt.title('QQ Plot')
plt.show()

# Perform statistical test for normality (Shapiro-Wilk test)
shapiro_test = stats.shapiro(residuals)
print(f"Shapiro-Wilk test p-value: {shapiro_test[1]}")  # p-value > 0.05 means normality

# Heteroscedasticity test (ARCH Test)
arch_test = het_arch(residuals)
print(f"ARCH test p-value: {arch_test[1]}")  # p-value > 0.05 means homoscedasticity (constant variance)

"""forcasting

"""

# Forecasting future values (e.g., for the test period or beyond)
forecast_steps = len(test)  # Number of future points to forecast (e.g., the length of the test data)
forecasted_values = best_arima_fit.forecast(steps=forecast_steps)

# Optionally: Forecast beyond the test period (e.g., 12 months into the future)
forecasted_values_future = best_arima_fit.forecast(steps=12)

# Print or plot forecasted values
print("Forecasted values:", forecasted_values)

from sklearn.metrics import mean_absolute_error, mean_squared_error

# Evaluate model performance
mae = mean_absolute_error(test, forecasted_values)
rmse = mean_squared_error(test, forecasted_values, squared=False)

# MAPE requires division by actual values (ignoring zeros to avoid division by zero)
mape = np.mean(np.abs((test - forecasted_values) / test)) * 100

print(f"MAE: {mae}")
print(f"RMSE: {rmse}")
print(f"MAPE: {mape}%")

import matplotlib.pyplot as plt

# Plot the actual and forecasted values
plt.figure(figsize=(10, 6))
plt.plot(train.index, train, label='Training Data')
plt.plot(test.index, test, label='Actual Test Data')
plt.plot(test.index, forecasted_values, label='Forecasted Values', linestyle='--')

# Optionally: Plot future forecast (if any)
plt.plot(forecasted_values_future.index, forecasted_values_future, label='Future Forecast', linestyle='--', color='green')

plt.title('Time Series Forecast')
plt.xlabel('Date')
plt.ylabel('Value')
plt.legend()
plt.show()

# Forecast 12 steps ahead (e.g., for monthly data, this would be 12 months)
forecasted_values_future = best_arima_fit.forecast(steps=12)
print("Forecast for the next 12 periods:", forecasted_values_future)

# Plot future forecast
plt.plot(forecasted_values_future, label='Future Forecast')
plt.title('Future Forecast (Next 12 Periods)')
plt.show()

# Load your dataset
#X =  'Date'
#y = 'Close'

from sklearn.model_selection import learning_curve

# Predictions
#y_pred = best_rf.predict(X_test.reshape(-1, 1)) # Reshape X_test to a 2D array
#y_pred_proba = best_rf.predict_proba(X_test.reshape(-1, 1))[:, 1] # Reshape X_test to a 2D array

# Calculate ROC curve and AUC
# Assuming y_test is multiclass and you want to plot ROC for class 1
#from sklearn.preprocessing import label_binarize

#y_test_bin = label_binarize(y_test, classes=np.unique(y_test))
#fpr, tpr, _ = roc_curve(y_test_bin[:, 1], y_pred_proba) # Use the probabilities for class 1
#roc_auc = auc(fpr, tpr)

# You can repeat this for other classes by changing the index in y_test_bin[:, class_index]

# Plot ROC curve
#plt.figure()
#plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
#plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
#plt.xlim([0.0, 1.0])
#plt.ylim([0.0, 1.05])
#plt.xlabel('False Positive Rate')
#plt.ylabel('True Positive Rate')
#plt.title('Receiver Operating Characteristic')
#plt.legend(loc="lower right")
#plt.show()

# Make predictions on the test set
#y_test_pred = best_rf.predict(X_test.reshape(-1, 1)) # Reshape X_test to a 2D array

# Classification Report
print(classification_report(y_test, y_test_pred))

# Accuracy score
print("Accuracy on test data: ", accuracy_score(y_test, y_test_pred))

from sklearn.model_selection import cross_val_score

plt.figure(figsize=(10, 6))
sns.boxplot(data=Data)
plt.title("Amazon stock Prices")
plt.xticks(rotation=45)
plt.show()

col = ['Date', 'Open', 'High', 'Low','Close', 'Adj Close', 'Volume']

for i in col:
  plt.figure()
  plt.boxplot(Data)
  plt.title(i)
  plt.show()

plt.figure(figsize=(10, 6))
plt.title("Boxplot")
plt.boxplot(numeric_Data)
plt.show()

# Convert the Pandas Series to a DataFrame.
Data = Data.to_frame()

# Now you can use the boxplot() method.
Data.boxplot()
plt.show()

Data.hist(figsize=(10, 6))
plt.tight_layout()
plt.show()

if 'Open' not in Data.columns:
  Data['Open'] = 0
Data['Open'] = pd.to_numeric(Data['Open'])

for i in Data.select_dtypes(include='number').columns:
  sns.histplot(data=Data, x=i)
  plt.show()

#Distribution of data set
numerical_cols = Data.select_dtypes(include=np.number).columns

for col in numerical_cols:
    sns.displot(Data[col],kde=True, fill=True)
    plt.title(f'Distribution of {col}') # This line should be indented inside the loop

plt.tight_layout() # This line and the next should be outside the loop.
plt.show()

# Print best parameters
print("Best parameters found: ", grid_search.best_params_)

from sklearn.preprocessing import StandardScaler

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import accuracy_score, classification_report, roc_curve, auc
import matplotlib.pyplot as plt

# Load your dataset
# Ensure X and y are assigned with the actual data, not the column names
X = Data.values # Access the values of the Series directly
y = Data.values # Access the values of the Series directly

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.model_selection import train_test_split

# Assuming 'Open' is your feature and you want to predict 'Open'
X = Data[['Open']]  # Define your features matrix using the existing 'Open' column
y = Data['Open']  # Define your target variable using the existing 'Open' column

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the model
rf = RandomForestClassifier(random_state=42)

# Define hyperparameter grid
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]}

from sklearn.ensemble import RandomForestRegressor

rfr = RandomForestRegressor()
rfr.fit(X_train, y_train)

y_pred = rfr.predict(X_test)
y_pred

from sklearn.metrics import mean_squared_error, r2_score,accuracy_score

r2 = r2_score(y_test, y_pred)
MSE = mean_squared_error(y_test, y_pred)
print("R-squared score:", r2)
print("Mean squared error:", MSE)

from sklearn.svm import SVR

svr = SVR()
svr.fit(X_train, y_train)

y_pred = svr.predict(X_test)
y_pred

from sklearn.metrics import mean_squared_error, r2_score

r2 = r2_score(y_test, y_pred)
MSE = mean_squared_error(y_test, y_pred)
print("R-squared score:", r2)
print("Mean squared error:", MSE)

import matplotlib.pyplot as plt
from sklearn.ensemble import GradientBoostingRegressor

# Instantiate the Gradient Boosting Regressor
gb_model = GradientBoostingRegressor(random_state=42)

# Fit the model to the training data
gb_model.fit(X_train, y_train)

# Predict on the test data
y_pred_gb = gb_model.predict(X_test)

# Evaluate the model
mse_gb = mean_squared_error(y_test, y_pred_gb)
r2_gb = r2_score(y_test, y_pred_gb)

print("Gradient Boosting Regressor:")
print("- Mean squared error:", mse_gb)
print("- R-squared score:", r2_gb)

import matplotlib.pyplot as plt
import xgboost as xgb

# Create an XGBoost regressor
xgb_model = xgb.XGBRegressor(random_state=42)

# Fit the model to the training data
xgb_model.fit(X_train, y_train)

# Predict on the test data
y_pred_xgb = xgb_model.predict(X_test)

# Evaluate the model
mse_xgb = mean_squared_error(y_test, y_pred_xgb)
r2_xgb = r2_score(y_test, y_pred_xgb)

print("XGBoost Regressor:")
print("- Mean squared error:", mse_xgb)
print("- R-squared score:", r2_xgb)

#  Support Vector Machine
from sklearn.svm import SVR #Import the SVR class for Support Vector Regression
svm_model = SVR() # Use SVR for regression
svm_model.fit(X_train, y_train)
svm_predictions = svm_model.predict(X_test)

# Evaluate the model using metrics suitable for regression
from sklearn.metrics import mean_squared_error, r2_score
mse = mean_squared_error(y_test, svm_predictions)
r2 = r2_score(y_test, svm_predictions)
print("SVM Mean Squared Error:", mse)
print("SVM R-squared score:", r2)

# Get the best parameters from the grid search
best_params = grid_search.best_params_

# Print the best parameters
print("Best Parameters:", best_params)

# Create a new model with the best parameters
best_model = RandomForestRegressor(**best_params)

# Fit the best model to the training data
best_model.fit(X_train, y_train)

# Fit the GridSearchCV object to the data
#grid_search.fit(X_train.reshape(-1, 1), y_train) # Reshape X_train to a 2D array if it has a single feature

# Create an SVR object with desired parameters (if needed)
svr = SVR(kernel='rbf', C=1.0, gamma='scale') # Example parameters

# Fit the model to the training data
svr.fit(X_train, y_train) # Added this line to fit the model

# Predict on the test data
y_pred = svr.predict(X_test)

# Evaluate the model using appropriate metrics for regression
from sklearn.metrics import mean_squared_error, r2_score
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print("SVM Mean Squared Error:", mse)
print("SVM R-squared score:", r2)

# Create GridSearchCV object
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3) # You can adjust the cv value as needed

from sklearn.model_selection import KFold
# Create GridSearchCV object with KFold
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=KFold(n_splits=3))

import pickle
with open('model.pkl', 'wb') as f:
  pickle.dump(best_model, f)